English | [ä¸­æ–‡](README.zh-CN.md)


# ArxivPaper for LLM

ArxivPaper is an arXiv paper triage and rapid-reading pipeline designed for research/engineering teams.
Within a specified time window, it pulls papers from arXiv, deduplicates them, then uses an LLM to score each paper for topic relevance and filters by a threshold. It then automatically downloads PDFs, generates preview pages, and extracts author affiliations from the previews to identify papers from â€œlarge institutions.â€ For the selected papers, it performs full-text parsing and generates Chinese summaries, and also produces a â€œcover page + result-figure digestâ€ (multi-page PNGs with the same dimensions as the cover) for fast visual skimming. Finally, it collects and archives all artifacts (PDFs, summaries, images, etc.) per paper; pushing to Zotero is an optional final step.

It produces two categories of outputs:

- Daily digest file (core deliverable): a single, ready-to-read report that aggregates summaries of the dayâ€™s selected papers (for quick daily scanning).

- Per-paper package (optional but generated by default): archived outputs per paper, including the PDF, Chinese summary/compressed summary, affiliation metadata, and the result-figure digest pages, for later review and deep reading.

Zotero export is optional and does not affect the generation of the daily digest.

> English README structure:
>
> 1. Configuration  2) Install & Run  3) Project Structure  4) Pipeline (step-by-step)

---

## 1. Configuration

Mainly edit `config/config.py`.

### 1.1 Required keys

- **MinerU Token** (`minerU_Token`): used for PDF â†’ Markdown via MinerU. Create at `https://mineru.net/apiManage/token`.
- **DashScope Key** (`qwen_api_key`): used for institution detection + summary generation via Qwen. Create at `https://bailian.console.aliyun.com/?spm=a2c4g.11186623.0.0.519511fceUTPZn&tab=model#/api-key`.

> For security, keep real keys in environment variables (`MINERU_TOKEN`, `QWEN_API_KEY`, etc.) or local untracked files. The repo version of `config.py` should keep these fields empty.

### 1.2 Model & prompt configs (meaning)

| Config key                     | Script                 | Meaning                                                        |
| ----------------------------- | ---------------------- | ------------------------------------------------------------- |
| `theme_select_base_url`       | `llm_select_theme.py`  | Base URL (OpenAI-compatible) for topic-scoring model          |
| `theme_select_model`          | `llm_select_theme.py`  | Topic-scoring model name                                      |
| `theme_select_max_tokens`     | `llm_select_theme.py`  | Max output tokens for scoring                                 |
| `theme_select_temperature`    | `llm_select_theme.py`  | Sampling temperature for scoring                              |
| `theme_select_concurrency`    | `llm_select_theme.py`  | Number of parallel workers for scoring                        |
| `theme_select_system_prompt`  | `llm_select_theme.py`  | System prompt for topic relevance scoring (0â€“1 score)         |
| `org_base_url`                | `pdf_info.py`          | Base URL for institution-detection model                      |
| `org_model`                   | `pdf_info.py`          | Institution model name                                        |
| `org_max_tokens`              | `pdf_info.py`          | Max output tokens for institution call                        |
| `org_temperature`             | `pdf_info.py`          | Sampling temperature for institution call                     |
| `pdf_info_system_prompt`      | `pdf_info.py`          | Rules for institution detection + â€œis_largeâ€ + short abstract |
| `summary_base_url`            | `paper_summary.py`     | Base URL for summary model                                    |
| `summary_model`               | `paper_summary.py`     | Summary model name                                            |
| `summary_max_tokens`          | `paper_summary.py`     | Max output tokens for summary                                 |
| `summary_temperature`         | `paper_summary.py`     | Sampling temperature for summary                              |
| `summary_input_hard_limit`    | `paper_summary.py`     | Hard input limit (for budget cutting)                         |
| `summary_input_safety_margin` | `paper_summary.py`     | Safety margin reserved for prompts/structure                  |
| `summary_concurrency`         | `paper_summary.py`     | Number of parallel workers for summary                        |
| `summary_example`             | `config.py`            | Example text used in the summary prompt                       |
| `system_prompt`               | `paper_summary.py`     | System prompt for summary (defines structure & style)         |

---

## 2. Install & Run

### 2.1 Install dependencies

```bash
pip install -r requirements.txt
```

> It is recommended to use a virtual environment, e.g.:
> - Unix/macOS: `python -m venv .venv && source .venv/bin/activate`  
> - Windows: `python -m venv .venv && .\.venv\Scripts\activate`

### 2.2 Run

#### 2.2.1 Run with default pipeline

```bash
python app.py
```

#### 2.2.2 Run with extra arguments (two examples)

```bash
# Example 1: natural language query + specify timezone
python app.py default --query "LLM alignment" --anchor-tz Asia/Shanghai
```

> Any arguments after the pipeline name (e.g. `default` / `daily`) are **only passed to Step 1** (`Controller/arxiv_search04.py`).

### 2.3 CLI options

| Argument          | Default                | Description                                                                 |
| ----------------- | ---------------------:| --------------------------------------------------------------------------- |
| `--query`         |                   `""`| Natural language or advanced query (ti:/abs:/AND/...)                       |
| `--categories`    | `SEARCH_CATEGORIES`   | Comma-separated arXiv category list                                         |
| `--start`         |                   `""`| UTC start (YYYY-MM-DD or ISO8601)                                          |
| `--end`           |                   `""`| UTC end (right-open; if date-only, automatically +1 day)                   |
| `--anchor-tz`     |  `Asia/Shanghai`      | Use 00:00 of this timezone as the default `end`                            |
| `--days`          |                    `1`| If no start/end: go back `days` from the anchor date                       |
| `--anchor-date`   |                   `""`| Anchor date (YYYY-MM-DD)                                                   |
| `--last-hours`    |                 `None`| If set, use `[now_utc - last_hours, now_utc]` as window (mutually exclusive)|
| `--page-size`     | `PAGE_SIZE_DEFAULT`   | Page size (1â€“2000)                                                         |
| `--max-papers`    | `MAX_PAPERS_DEFAULT`  | Max number of papers to keep                                               |
| `--sleep`         |   `SLEEP_DEFAULT`     | Sleep between pages (seconds)                                              |
| `--use-proxy`     | `USE_PROXY_DEFAULT`   | Allow reading proxy from environment                                       |
| `--user-agent`    |       `REQUESTS_UA`   | HTTP User-Agent                                                            |
| `--out`           |          (reserved)   | Present in CLI but unused in current version                               |

---

## 3. Project Structure

```markdown
. ğŸ“‚ ArxivPaper                        # Project root
â”œâ”€â”€ ğŸ“„ README.md                       # English README (this file)
â”œâ”€â”€ ğŸ“„ README.zh-CN.md                 # Chinese README
â”œâ”€â”€ ğŸ“„ app.py                          # Main entry: orchestrates Controller steps by pipeline
â”œâ”€â”€ ğŸ“„ pdf_download.log                # Log file of pdf_download.py
â”œâ”€â”€ ğŸ“„ readmePrinceple.md              # Notes & principles for writing the README
â”œâ”€â”€ ğŸ“‚ Controller/                     # Core pipeline step scripts
â”‚  â”œâ”€â”€ ğŸ“‚ __pycache__/                 # Python bytecode cache
â”‚  â”œâ”€â”€ ğŸ“„ arxiv_search04.py            # Step 1: arXiv fetch (query + time window)
â”‚  â”œâ”€â”€ ğŸ“„ llm_select_theme.py          # Step 2: LLM topic-relevance scoring
â”‚  â”œâ”€â”€ ğŸ“„ paper_theme_filter.py        # Step 3: filter by relevance score
â”‚  â”œâ”€â”€ ğŸ“„ http_session.py              # Shared requests Session + retry logic
â”‚  â”œâ”€â”€ ğŸ“„ instutions_filter.py         # Step 6: filter â€œlarge-institutionâ€ papers
â”‚  â”œâ”€â”€ ğŸ“„ paperList_remove_duplications.py # Step 1.1: de-duplicate & track processed papers
â”‚  â”œâ”€â”€ ğŸ“„ paper_summary.py             # Step 11: generate Chinese summaries from full MinerU md
â”‚  â”œâ”€â”€ ğŸ“„ pdf_download.py              # Step 4: download raw PDFs (per date subdir)
â”‚  â”œâ”€â”€ ğŸ“„ pdf_info.py                  # Step 7: LLM-based institution recognition & abstract
â”‚  â”œâ”€â”€ ğŸ“„ pdf_split.py                 # Step 5: cut preview PDFs (first N pages)
â”‚  â”œâ”€â”€ ğŸ“„ pdfsplite_to_minerU.py       # Step 6: preview PDFs â†’ MinerU Markdown
â”‚  â”œâ”€â”€ ğŸ“„ selectedpaper_to_mineru.py   # Step 10: selected PDFs â†’ full MinerU Markdown
â”‚  â”œâ”€â”€ ğŸ“„ selectpaper.py               # Step 9: move â€œlarge institutionâ€ PDFs to selected dir
â”‚  â”œâ”€â”€ ğŸ“„ summary_limit.py             # Step 11.5: compress summaries into fixed structure
â”‚  â”œâ”€â”€ ğŸ“„ zotero_push.py               # Step 12: push selected papers into Zotero
â”œâ”€â”€ ğŸ“‚ config/                         # Central configuration
â”‚  â”œâ”€â”€ ğŸ“‚ __pycache__/                 # Bytecode cache
â”‚  â”œâ”€â”€ ğŸ“„ config copy.py               # Old backup of config (for reference only)
â”‚  â”œâ”€â”€ ğŸ“„ paperList.json               # Global list of processed papers (for de-dup)
â”œâ”€â”€ ğŸ“‚ data/                           # Runtime data (organized by date)
â”‚  â”œâ”€â”€ ğŸ“‚ arxivList/                   # Daily candidate lists (md/json)
â”‚  â”‚  â”œâ”€â”€ ğŸ“‚ md/                       # Candidate Markdown lists
â”‚  â”‚  â””â”€â”€ ğŸ“‚ json/                     # Candidate JSON lists
â”‚  â”œâ”€â”€ ğŸ“‚ paperList_remove_duplications/ # De-duplicated candidate JSON
â”‚  â”œâ”€â”€ ğŸ“‚ llm_select_theme/            # JSON after LLM topic scoring
â”‚  â”œâ”€â”€ ğŸ“‚ paper_theme_filter/          # JSON after topic filtering
â”‚  â”œâ”€â”€ ğŸ“‚ raw_pdf/                     # Raw PDFs + manifest
â”‚  â”œâ”€â”€ ğŸ“‚ preview_pdf/                 # Preview PDFs + manifest
â”‚  â”œâ”€â”€ ğŸ“‚ preview_pdf_to_mineru/       # MinerU md + manifest for previews
â”‚  â”œâ”€â”€ ğŸ“‚ pdf_info/                    # Institution recognition JSON
â”‚  â”œâ”€â”€ ğŸ“‚ instutions_filter/           # â€œlarge institutionâ€ paper lists
â”‚  â”œâ”€â”€ ğŸ“‚ selectedpaper/               # Selected PDFs + manifest
â”‚  â”œâ”€â”€ ğŸ“‚ selectedpaper_to_mineru/     # Full MinerU md + manifest for selected PDFs
â”‚  â”œâ”€â”€ ğŸ“‚ paper_summary/               # Summaries (single + daily gather)
â”‚  â”œâ”€â”€ ğŸ“‚ summary_limit/               # Compressed summaries (single + daily gather)
â”‚  â”œâ”€â”€ ğŸ“‚ select_image/                # Summary image pages (PNG) + report
â”‚  â””â”€â”€ ğŸ“‚ file_collect/                # Final collected packages by paper id
â”œâ”€â”€ ğŸ“‚ logs/                           # Runtime logs (by date)
â””â”€â”€ ğŸ“‚ reference/                      # Reference projects & sample code (copied from old repo)
```

---

## 4. Pipeline (step-by-step)

**Default pipeline in `app.py`**

- 1) Fetch arXiv & time-window filter (`arxiv_search04.py`)
- 2) De-duplicate & record processed papers (`paperList_remove_duplications.py`)
- 3) LLM topic relevance scoring (`llm_select_theme.py`)
- 4) Topic-based filtering (`paper_theme_filter.py`)
- 5) Download raw PDFs (`pdf_download.py`)
- 6) Cut preview PDFs (`pdf_split.py`)
- 7) Preview PDFs â†’ MinerU Markdown (`pdfsplite_to_minerU.py`)
- 8) Institution detection & structured info (`pdf_info.py`)
- 9) Build â€œlarge institutionâ€ PDF list (`instutions_filter.py`)
- 10) Move selected PDFs (`selectpaper.py`)
- 11) Full selected PDFs â†’ MinerU Markdown (`selectedpaper_to_mineru.py`)
- 12) Generate Chinese summaries (`paper_summary.py`)
- 13) Compress summaries by section (`summary_limit.py`)
- 14) Generate summary image pages (`select_image.py`)
- 15) Collect all artifacts per paper (`file_collect.py`)
- 16) Push selected papers into Zotero (`zotero_push.py`)

### 0) Orchestration (`app.py`)

- **Input**: pipeline name + extra CLI arguments.
- **Output**: sequential execution of step scripts (`Controller/*.py`, see steps 1â€“16).

**Logic**

- Read pipeline (default: `default`).
- Execute each step via `subprocess.run()` in order.
- Forward arguments *after* the pipeline name only to Step 1 (`arxiv_search04.py`).

---

### 1) Fetch arXiv & time-window filter (`Controller/arxiv_search04.py`)

- **Input**
  - arXiv search conditions (`SEARCH_CATEGORIES` + `--query`)
  - Time window & scale (`--start/--end/--anchor-*` + `PAGE_SIZE_DEFAULT/MAX_PAPERS_DEFAULT/SLEEP_DEFAULT`)
- **Output**
  - Daily candidate Markdown (`data/arxivList/md/<date>.md`)
  - Daily candidate JSON (`data/arxivList/json/<date>.json`)
- **Logic**
  - Compute UTC window (`submittedDate:[START TO END]`)
  - Build query: `(cat:... OR ...) AND (all:... OR advanced expr) AND submittedDate`
  - Fetch pages in `submittedDate desc` order
  - Only filter by time window (no regex scoring/bucketing)
  - Output Markdown with window info & statistics, ordered list of papers

---

### 2) De-duplicate & record processed papers (`Controller/paperList_remove_duplications.py`)

- **Input**
  - Daily candidate JSON (`data/arxivList/json/<date>.json`, latest by default)
  - History file (`config/paperList.json`, may be empty on first run)
- **Output**
  - Updated history (`config/paperList.json`, JSON array)
  - De-duplicated list (`data/paperList_remove_duplications/<date>.json`)
  - Each history record:
    - `title`: paper title
    - `source`: paper id (e.g. `2601.02454`)
    - `writing_datetime`: UTC ISO timestamp
- **Logic**
  - Load existing history and build a `(title, source)` key set
  - For each paper in the daily candidate list:
    - If key exists in history â†’ consider â€œalready processedâ€, skip
    - Else â†’ append `{title, source, writing_datetime}` to history
  - Write a new JSON keeping only â€œnot-seen-beforeâ€ papers, preserving meta

> If you want later steps to only work on â€œnewâ€ papers, pass  
> `--json data/paperList_remove_duplications/<date>.json` to `Controller/pdf_download.py`.

---

### 3) LLM topic relevance scoring (`Controller/llm_select_theme.py`)

- **Input**
  - De-duplicated list (`data/paperList_remove_duplications/<date>.json`)
  - Scoring model config (`theme_select_*`)
- **Output**
  - Scored list (`data/llm_select_theme/<date>.json`) with extra field `theme_relevant_score`
- **Logic**
  - Parse title & abstract of each paper
  - Call LLM in parallel to get a 0â€“1 relevance score
  - Write back original structure plus score

---

### 4) Topic-based filtering (`Controller/paper_theme_filter.py`)

- **Input**: scored list (`data/llm_select_theme/<date>.json`)
- **Output**: filtered list (`data/paper_theme_filter/<date>.json`)
- **Logic**
  - Read `theme_relevant_score`
  - Keep only entries with `score >= threshold` (preserving head order)

---

### 5) Download raw PDFs (`Controller/pdf_download.py`)

- **Input**: filtered list (`data/paper_theme_filter/<date>.json`)
- **Output**
  - Raw PDFs (`data/raw_pdf/<date>/<arxiv_id>.pdf`)
  - Download manifest (`data/raw_pdf/<date>/_manifest.json`)
- **Logic**
  - Parse arXiv id for each entry
  - If a local file exists and starts with `%PDF-`, treat as valid and skip
  - Otherwise, download with retries, write `.part`, basic validation, then atomically rename

---

### 6) Cut preview PDFs (`Controller/pdf_split.py`)

- **Input**: raw PDFs (`data/raw_pdf/<date>/<arxiv_id>.pdf`)
- **Output**
  - Preview PDFs (first 2 pages): `data/preview_pdf/<date>/<arxiv_id>.pdf`
  - Split manifest: `data/preview_pdf/<date>/_manifest.json`
- **Logic**
  - For each raw PDF, cut first N pages into preview dir; skip if already exists

---

### 7) Preview PDF â†’ MinerU Markdown (`Controller/pdfsplite_to_minerU.py`)

- **Input**
  - Preview PDFs / manifest (`data/preview_pdf/<date>/*.pdf`, `_manifest.json`)
  - MinerU token (`minerU_Token`)
- **Output**
  - Preview Markdown (`data/preview_pdf_to_mineru/<date>/<arxiv_id>.md`)
  - MinerU manifest (`data/preview_pdf_to_mineru/<date>/_manifest.json`)
- **Logic**
  - MinerU batch flow: request upload URL â†’ PUT upload â†’ poll result â†’ download zip â†’ extract md
  - Skip a paper if its output md already exists

---

### 8) Institution detection & structured info (`Controller/pdf_info.py`)

- **Input**
  - Preview MinerU md (`data/preview_pdf_to_mineru/<date>/*.md`)
  - Meta info from filtered JSON (`data/paper_theme_filter/<date>.json`)
  - Institution model + prompts (`org_*`, `pdf_info_system_prompt`)
- **Output**
  - Structured JSON (`data/pdf_info/<date>.json`) with fields like `instution`, `is_large`, `abstract`
- **Logic**
  - Call the LLM in parallel (default concurrency=8, configurable in `config/config.py`)
  - Merge title / published / arxiv_id etc. into the result; deduplicate by arxiv_id

---

### 9) Build â€œlarge institutionâ€ PDF list (`Controller/instutions_filter.py`)

- **Input**: structured results (`data/pdf_info/<date>.json`)
- **Output**: JSON list of large-institution papers (`data/instutions_filter/<date>/<date>.json`)
- **Logic**
  - Filter entries with `is_large == true` and write out for later PDF moving

---

### 10) Move selected PDFs (`Controller/selectpaper.py`)

- **Input**
  - Large-institution list (`data/instutions_filter/<date>/<date>.json`)
  - Raw PDFs (`data/raw_pdf/<arxiv_id>.pdf`)
- **Output**
  - Selected PDFs (`data/selectedpaper/<date>/<arxiv_id>.pdf`)
  - Move manifest (`data/selectedpaper/<date>/_manifest.json`)
- **Logic**
  - Parse arxiv_id from list and `shutil.move` PDFs into selected dir (source file removed)

---

### 11) Selected PDF â†’ full MinerU Markdown (`Controller/selectedpaper_to_mineru.py`)

- **Input**
  - Selected PDFs / manifest (`data/selectedpaper/<date>/*.pdf`, `_manifest.json`)
  - MinerU token (`minerU_Token`)
- **Output**
  - Full Markdown (`data/selectedpaper_to_mineru/<date>/<arxiv_id>.md`)
  - MinerU manifest (`data/selectedpaper_to_mineru/<date>/_manifest.json`)
- **Logic**
  - Run MinerU full-document parsing; skip if output md already exists

---

### 12) Generate Chinese summaries (`Controller/paper_summary.py`)

- **Input**
  - Full MinerU md (`data/selectedpaper_to_mineru/<date>/*.md`)
  - Summary model & prompts (`summary_*`, `system_prompt`)
- **Output**
  - Per-paper summaries (`data/paper_summary/single/<date>/<arxiv_id>.md`)
  - Daily gather file (`data/paper_summary/gather/<date>/<date>.txt`)
- **Logic**
  - Cut input according to model context budget
  - Call LLM in parallel and write per-paper md
  - Concatenate per-paper outputs into a daily gather file

---

### 13) Compress summaries (`Controller/summary_limit.py`)

- **Input**
  - Per-paper summaries (`data/paper_summary/single/<date>/<arxiv_id>.md`)
  - Compression model & prompts (`summary_limit_*`, `summary_limit_prompt_*`)
- **Output**
  - Compressed per-paper summaries (`data/summary_limit/single/<date>/<arxiv_id>.md`)
  - Daily gather file (`data/summary_limit/gather/<date>/<date>.txt`)
- **Logic**
  - If total length â‰¤ 950 (non-whitespace chars), copy as-is
  - Otherwise split into sections: intro / key ideas / findings / opinion
  - For each section exceeding its limit, call the corresponding prompt to rewrite & shrink
  - Reassemble into the original structure and generate a daily gather file
  - Overwrite title & source using `pdf_info/<date>.json`
  - Enforce bullet style (`ğŸ”¸`) and item counts for each section

---

### 14) Summary image pages (`Controller/select_image.py`)

- **Goal**: render page 1 of the PDF as a cover, then select â€œresult-relatedâ€ figures from MinerU output and layout multi-page PNGs with the same size as the cover.
- **Input**
  - Selected PDFs (`data/selectedpaper/<date>/<arxiv_id>.pdf`)
  - Full MinerU output (`data/selectedpaper_to_mineru/<date>/<arxiv_id>/...`, including `*.md` / images / `*_content_list.json`)
- **Output**
  - Summary PNG pages (`data/select_image/<date>/<arxiv_id>/01.png` as cover, `02.png...` as result pages)
  - Selection report (`data/select_image/<date>/<arxiv_id>/report.json`)
- **Logic**
  - Extract the first page of the selected PDF as the cover
  - Select â€œresult-relatedâ€ figures based on content list and keyword matching
  - Layout as multi-page PNGs using configured engine (HTML/CSS or ReportLab)
  - Generate a statistics report
- **Run examples**

```bash
# Option A: HTML/CSS layout â†’ Chromium print to PDF â†’ render to PNG (more stable)
python Controller/select_image.py --layout-engine html
```

> Requires Playwright + Chromium:
>
> ```bash
> pip install playwright
> python -m playwright install chromium
> ```
>
> If dependencies are missing, the script will fall back to the older PIL layout (`--layout-engine pil`).

```bash
# Option B: Justified rows layout â†’ ReportLab â†’ PNG
python Controller/select_image.py --layout-engine reportlab
```

> Requires ReportLab:
>
> ```bash
> pip install reportlab
> ```

---

### 15) Collect files (`Controller/file_collect.py`)

- **Input**
  - Selected PDFs (`data/selectedpaper/<date>/<arxiv_id>.pdf`)
  - Chinese summaries (`data/paper_summary/single/<date>/<arxiv_id>.md`)
  - Compressed summaries (`data/summary_limit/single/<date>/<arxiv_id>.md`)
  - Institution info (`data/pdf_info/<date>.json`)
  - Summary images (`data/select_image/<date>/<arxiv_id>/*.png`)
- **Output**
  - Per-paper final bundle (`data/file_collect/<date>/<arxiv_id>/`)
    - `{arxiv_id}.pdf`: selected PDF
    - `{arxiv_id}_summary.md`: Chinese summary
    - `{arxiv_id}_limit.md`: compressed summary
    - `pdf_info.json`: institution JSON
    - `image/01.png, 02.png...`: summary PNGs (0X.png)
- **Logic**
  - Locate selected PDFs and related files by date
  - Create a dedicated output directory for each paper
  - Copy PDFs, summaries, and institution JSON into the directory
  - Copy summary PNGs into an `image/` subdir
  - Record missing files and print statistics
- **Run examples**

```bash
# Use latest or today as default date
python Controller/file_collect.py

# Specify date explicitly
python Controller/file_collect.py --date 2025-01-26
```

---

### 16) Push into Zotero (`Controller/zotero_push.py`)

- **Input**
  - Selected PDFs (`data/selectedpaper/<date>/*.pdf`)
  - Chinese summaries (`data/paper_summary/single/<date>/*.md`)
- **Output**
  - Items and attachments created in Zotero (no extra local files)
- **Logic**
  - Locate selected PDFs and summary directory by date
  - Build Zotero items (title, summary, arXiv link, etc.)
  - Use Zotero Connector `/connector/saveItems` to create items
  - Use `/connector/saveAttachment` to upload PDF/MD/summary as attachments
  - Show progress in a single-line fashion and print final statistics

[English](README.md) | ä¸­æ–‡

