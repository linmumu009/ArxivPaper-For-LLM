from __future__ import annotations

import argparse
import json
import os
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Dict

from openai import OpenAI

import sys

sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config.config import (  # noqa: E402
    qwen_api_key,
    summary_limit_base_url,
    summary_limit_model,
    summary_limit_max_tokens,
    summary_limit_temperature,
    summary_limit_input_hard_limit,
    summary_limit_input_safety_margin,
    summary_limit_concurrency,
    summary_limit_section_limit_intro,
    summary_limit_section_limit_method,
    summary_limit_section_limit_findings,
    summary_limit_section_limit_opinion,
    summary_limit_headline_limit,
    summary_limit_prompt_intro,
    summary_limit_prompt_method,
    summary_limit_prompt_findings,
    summary_limit_prompt_opinion,
    summary_limit_prompt_structure_check,
    summary_limit_prompt_structure_rewrite,
    summary_limit_prompt_headline,
    DATA_ROOT,
)


SECTION_LABELS = {
    "intro": ("ðŸ›Žï¸æ–‡ç« ç®€ä»‹", "æ–‡ç« ç®€ä»‹"),
    "method": ("ðŸ“é‡ç‚¹æ€è·¯", "é‡ç‚¹æ€è·¯"),
    "findings": ("ðŸ”Žåˆ†æžæ€»ç»“", "åˆ†æžæ€»ç»“"),
    "opinion": ("ðŸ’¡ä¸ªäººè§‚ç‚¹", "ä¸ªäººè§‚ç‚¹"),
}

SECTION_LIMITS = {
    "intro": summary_limit_section_limit_intro,
    "method": summary_limit_section_limit_method,
    "findings": summary_limit_section_limit_findings,
    "opinion": summary_limit_section_limit_opinion,
}

SECTION_PROMPTS = {
    "intro": summary_limit_prompt_intro,
    "method": summary_limit_prompt_method,
    "findings": summary_limit_prompt_findings,
    "opinion": summary_limit_prompt_opinion,
}


def approx_input_tokens(text: str) -> int:
    if not text:
        return 0
    return len(text.encode("utf-8", errors="ignore"))


def crop_to_input_tokens(text: str, limit_tokens: int) -> str:
    budget = int(limit_tokens)
    if budget <= 0:
        return ""
    b = text.encode("utf-8", errors="ignore")
    if len(b) <= budget:
        return text
    return b[:budget].decode("utf-8", errors="ignore")


def list_md_files(root: Path) -> List[Path]:
    return sorted(root.glob("*.md"))


def today_str() -> str:
    return datetime.now().date().isoformat()


def write_gather(single_dir: Path, gather_dir: Path, date_str: str) -> Path:
    files = list_md_files(single_dir)
    gather_dir.mkdir(parents=True, exist_ok=True)
    gather_path = gather_dir / f"{date_str}.txt"
    with gather_path.open("w", encoding="utf-8") as f:
        first = True
        for p in files:
            text = p.read_text(encoding="utf-8", errors="ignore").strip()
            if not text:
                continue
            if not first:
                f.write("\n")
            first = False
            f.write("#" * 100 + "\n")
            f.write(f"{p.name}\n")
            f.write("#" * 100 + "\n")
            f.write(text)
            f.write("\n")
    return gather_path


def make_client() -> OpenAI:
    key = (qwen_api_key or "").strip()
    if not key:
        raise SystemExit("qwen_api_key missing in config.config")
    base = (summary_limit_base_url or "").strip()
    if not base:
        raise SystemExit("summary_limit_base_url missing in config.config")
    return OpenAI(api_key=key, base_url=base)


def non_ws_len(text: str) -> int:
    return len(re.sub(r"\s+", "", text))


def normalize_heading(line: str) -> str:
    raw = line.strip()
    raw = re.sub(r"^#+\s*", "", raw)
    raw = re.sub(r"^[^\w\u4e00-\u9fff]+", "", raw)
    raw = raw.lstrip(":ï¼š- ").strip()
    return raw


def heading_key(line: str) -> Optional[str]:
    norm = normalize_heading(line)
    for key, labels in SECTION_LABELS.items():
        if norm.startswith(labels[0]) or norm.startswith(labels[1]):
            return key
    return None


def split_sections(lines: List[str]) -> Tuple[List[str], List[Tuple[str, str, List[str]]]]:
    prefix: List[str] = []
    sections: List[Tuple[str, str, List[str]]] = []
    current_key: Optional[str] = None
    current_heading: str = ""
    current_lines: List[str] = []

    for line in lines:
        key = heading_key(line)
        if key:
            if current_key:
                sections.append((current_key, current_heading, current_lines))
            elif current_lines:
                prefix.extend(current_lines)
            current_key = key
            current_heading = line
            current_lines = []
            continue
        if current_key is None:
            prefix.append(line)
        else:
            current_lines.append(line)

    if current_key:
        sections.append((current_key, current_heading, current_lines))
    return prefix, sections


def ensure_section_spacing(text: str) -> str:
    if not text.strip():
        return text
    lines = text.splitlines()
    out: List[str] = []
    for line in lines:
        if heading_key(line):
            if out and out[-1].strip():
                out.append("")
        out.append(line)
    return "\n".join(out).rstrip() + "\n"


def normalize_style(text: str) -> str:
    lines = text.splitlines()
    out: List[str] = []
    i = 0
    while i < len(lines):
        raw = lines[i].strip()
        if not raw:
            out.append("")
            i += 1
            continue
        if re.match(r"^-{3,}\s*$", raw):
            i += 1
            continue
        line = re.sub(r"^#+\s*", "", raw).strip()
        line = re.sub(r"\*\*(.*?)\*\*", r"\1", line)
        line = line.replace("ï¼š", ":")

        m = re.match(r"^(?:ðŸ“–\s*)?æ ‡é¢˜\s*:\s*(.+)$", line, re.IGNORECASE)
        if m:
            out.append(f"ðŸ“–æ ‡é¢˜ï¼š{m.group(1).strip()}")
            i += 1
            continue
        m = re.match(r"^(?:ðŸŒ\s*)?(?:æ¥æº|source)\s*:\s*(.+)$", line, re.IGNORECASE)
        if m:
            out.append(f"ðŸŒæ¥æºï¼š{m.group(1).strip()}")
            i += 1
            continue
        m = re.match(r"^(?:æœºæž„|ä½œè€…æœºæž„|å•ä½|æœºæž„å)\s*:\s*(.+)$", line, re.IGNORECASE)
        if m:
            out.append(f"{m.group(1).strip()}")
            i += 1
            continue

        if re.match(r"^(?:æœºæž„|ä½œè€…æœºæž„|å•ä½|æœºæž„å)$", line, re.IGNORECASE):
            content = ""
            j = i + 1
            while j < len(lines):
                candidate = lines[j].strip()
                if candidate:
                    content = re.sub(r"\*\*(.*?)\*\*", r"\1", candidate)
                    break
                j += 1
            out.append(content)
            i = j + 1
            continue
        if re.match(r"^æ ‡é¢˜$", line, re.IGNORECASE):
            content = ""
            j = i + 1
            while j < len(lines):
                candidate = lines[j].strip()
                if candidate:
                    content = re.sub(r"\*\*(.*?)\*\*", r"\1", candidate)
                    break
                j += 1
            out.append(f"ðŸ“–æ ‡é¢˜ï¼š{content}" if content else "ðŸ“–æ ‡é¢˜ï¼š")
            i = j + 1
            continue
        if re.match(r"^(?:æ¥æº|source)$", line, re.IGNORECASE):
            content = ""
            j = i + 1
            while j < len(lines):
                candidate = lines[j].strip()
                if candidate:
                    content = re.sub(r"\*\*(.*?)\*\*", r"\1", candidate)
                    break
                j += 1
            out.append(f"ðŸŒæ¥æºï¼š{content}" if content else "ðŸŒæ¥æºï¼š")
            i = j + 1
            continue

        key = heading_key(line)
        if key == "intro":
            if out and out[-1].strip():
                out.append("")
            out.append("ðŸ›Žï¸æ–‡ç« ç®€ä»‹")
            i += 1
            continue
        if key == "method":
            if out and out[-1].strip():
                out.append("")
            out.append("ðŸ“é‡ç‚¹æ€è·¯")
            i += 1
            continue
        if key == "findings":
            if out and out[-1].strip():
                out.append("")
            out.append("ðŸ”Žåˆ†æžæ€»ç»“")
            i += 1
            continue
        if key == "opinion":
            if out and out[-1].strip():
                out.append("")
            out.append("ðŸ’¡ä¸ªäººè§‚ç‚¹")
            i += 1
            continue

        if re.match(r"^(?:[-*â€¢]|ðŸ”¹|ðŸ”¸)\s*", line) or re.match(r"^\d+[.)]\s*", line):
            content = re.sub(r"^(?:[-*â€¢]|ðŸ”¹|ðŸ”¸)\s*", "", line)
            content = re.sub(r"^\d+[.)]\s*", "", content)
            content = re.sub(r"\*\*(.*?)\*\*", r"\1", content).strip()
            if content:
                out.append(f"ðŸ”¸{content}")
            i += 1
            continue

        out.append(line)
        i += 1
    return "\n".join(out).strip() + "\n"


def rewrite_block(client: OpenAI, text: str, sys_prompt: str, limit_chars: int, max_retries: int = 3) -> str:
    content = text.strip()
    if not content:
        return content
    for _ in range(max_retries):
        hard_limit = int(summary_limit_input_hard_limit)
        safety_margin = int(summary_limit_input_safety_margin)
        limit_total = hard_limit - safety_margin
        sys_tokens = approx_input_tokens(sys_prompt)
        user_budget = max(1, limit_total - sys_tokens)
        user_content = crop_to_input_tokens(content, user_budget)
        kwargs = {}
        if summary_limit_temperature is not None:
            kwargs["temperature"] = float(summary_limit_temperature)
        if summary_limit_max_tokens is not None:
            kwargs["max_tokens"] = int(summary_limit_max_tokens)
        resp = client.chat.completions.create(
            model=summary_limit_model,
            messages=[
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_content},
            ],
            stream=False,
            **kwargs,
        )
        new_text = resp.choices[0].message.content if resp.choices else ""
        if not new_text:
            new_text = content
        content = new_text.strip()
        if non_ws_len(content) <= limit_chars:
            break
    return content


def compress_headline(client: OpenAI, text: str) -> str:
    sys_prompt = (summary_limit_prompt_headline or "").strip()
    content = text.strip()
    if not sys_prompt or not content:
        return text
    hard_limit = int(summary_limit_input_hard_limit)
    safety_margin = int(summary_limit_input_safety_margin)
    limit_total = hard_limit - safety_margin
    sys_tokens = approx_input_tokens(sys_prompt)
    user_budget = max(1, limit_total - sys_tokens)
    user_content = crop_to_input_tokens(content, user_budget)
    resp = client.chat.completions.create(
        model=summary_limit_model,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_content},
        ],
        stream=False,
        max_tokens=summary_limit_max_tokens or 2048,
        temperature=0,
    )
    new_text = resp.choices[0].message.content if resp.choices else ""
    return new_text.strip() if new_text else text


def apply_headline_limit(client: OpenAI, lines: List[str]) -> List[str]:
    title_idx = None
    for idx, line in enumerate(lines):
        if line.strip().startswith("ðŸ“–æ ‡é¢˜"):
            title_idx = idx
            break
    if title_idx is None:
        return lines
    for prev_idx in range(title_idx - 1, -1, -1):
        candidate = lines[prev_idx].strip()
        if not candidate:
            continue
        if non_ws_len(candidate) <= summary_limit_headline_limit:
            return lines
        lines[prev_idx] = compress_headline(client, candidate) + "\n"
        return lines
    return lines


def extract_arxiv_id(source: str) -> Optional[str]:
    if not source:
        return None
    m = re.search(r"(\d{4}\.\d{4,5})(v\d+)?", source)
    if not m:
        return None
    version = m.group(2) or ""
    return f"{m.group(1)}{version}"


def load_pdf_info_map(date_str: str) -> Dict[str, Dict[str, str]]:
    info_path = Path(DATA_ROOT) / "pdf_info" / f"{date_str}.json"
    if not info_path.exists():
        return {}
    try:
        data = json.loads(info_path.read_text(encoding="utf-8", errors="ignore"))
    except json.JSONDecodeError:
        return {}
    if not isinstance(data, list):
        return {}
    out: Dict[str, Dict[str, str]] = {}
    for item in data:
        if not isinstance(item, dict):
            continue
        source = str(item.get("source", "") or "")
        arxiv_id = extract_arxiv_id(source)
        if not arxiv_id:
            continue
        out[arxiv_id] = item
    return out


def inject_pdf_info(text: str, md_path: Path, pdf_info_map: Dict[str, Dict[str, str]]) -> str:
    if not text.strip() or not pdf_info_map:
        return text
    key = md_path.stem
    info = pdf_info_map.get(key)
    if info is None:
        key_no_version = re.sub(r"v\d+$", "", key)
        info = pdf_info_map.get(key_no_version)
    if not info:
        return text

    title = str(info.get("title", "") or "").strip()
    source = str(info.get("source", "") or "").strip()
    instution = str(info.get("instution", "") or "").strip()

    lines = text.splitlines()
    first_idx = None
    for idx, line in enumerate(lines):
        if line.strip():
            first_idx = idx
            break
    if first_idx is None:
        first_idx = 0
        lines.insert(0, "ç¬”è®°æ ‡é¢˜ï¼š")

    first_line = lines[first_idx].strip()
    if instution:
        if first_line.startswith("ç¬”è®°æ ‡é¢˜"):
            rest = first_line[len("ç¬”è®°æ ‡é¢˜"):].lstrip("ï¼š:")
            lines[first_idx] = f"{instution}ï¼š{rest}".rstrip()
        elif first_line.startswith("æ ‡é¢˜"):
            rest = first_line[len("æ ‡é¢˜"):].lstrip("ï¼š:")
            lines[first_idx] = f"{instution}ï¼š{rest}".rstrip()
        else:
            lines[first_idx] = f"{instution}ï¼š{first_line}".rstrip()

    # Remove existing title/source lines before the first section header
    top_end = len(lines)
    for idx, line in enumerate(lines):
        if heading_key(line):
            top_end = idx
            break
    filtered: List[str] = []
    for idx, line in enumerate(lines):
        if idx < top_end:
            s = line.strip()
            if s.startswith("ðŸ“–æ ‡é¢˜") or s.startswith("æ ‡é¢˜") or s.startswith("ðŸŒæ¥æº") or s.lower().startswith("source") or s.startswith("æ¥æº"):
                continue
        filtered.append(line)
    lines = filtered

    insert_lines: List[str] = []
    if title:
        insert_lines.append(f"ðŸ“–æ ‡é¢˜ï¼š{title}")
    if source:
        insert_lines.append(f"ðŸŒæ¥æºï¼š{source}")

    if insert_lines:
        insert_at = min(first_idx + 1, len(lines))
        lines[insert_at:insert_at] = insert_lines

    return "\n".join(lines).rstrip() + "\n"


def structure_matches_example(client: OpenAI, text: str) -> bool:
    sys_prompt = (summary_limit_prompt_structure_check or "").strip()
    if not sys_prompt:
        return True
    content = text.strip()
    if not content:
        return False
    hard_limit = int(summary_limit_input_hard_limit)
    safety_margin = int(summary_limit_input_safety_margin)
    limit_total = hard_limit - safety_margin
    sys_tokens = approx_input_tokens(sys_prompt)
    user_budget = max(1, limit_total - sys_tokens)
    user_content = crop_to_input_tokens(content, user_budget)
    resp = client.chat.completions.create(
        model=summary_limit_model,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_content},
        ],
        stream=False,
        max_tokens=8,
        temperature=0,
    )
    reply = resp.choices[0].message.content.strip().upper() if resp.choices else ""
    return reply.startswith("YES")


def restructure_to_example(client: OpenAI, text: str) -> str:
    sys_prompt = (summary_limit_prompt_structure_rewrite or "").strip()
    if not sys_prompt:
        return text
    content = text.strip()
    if not content:
        return text
    hard_limit = int(summary_limit_input_hard_limit)
    safety_margin = int(summary_limit_input_safety_margin)
    limit_total = hard_limit - safety_margin
    sys_tokens = approx_input_tokens(sys_prompt)
    user_budget = max(1, limit_total - sys_tokens)
    user_content = crop_to_input_tokens(content, user_budget)
    resp = client.chat.completions.create(
        model=summary_limit_model,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_content},
        ],
        stream=False,
        max_tokens=summary_limit_max_tokens or 2048,
        temperature=0,
    )
    new_text = resp.choices[0].message.content if resp.choices else ""
    return new_text.strip() if new_text else text


def process_one(
    client: OpenAI,
    md_path: Path,
    out_path: Path,
    pdf_info_map: Dict[str, Dict[str, str]],
) -> Tuple[Path, str]:
    text = md_path.read_text(encoding="utf-8", errors="ignore")
    if not text.strip():
        out_path.write_text("", encoding="utf-8")
        return md_path, ""
    status = "copied"
    text = inject_pdf_info(text, md_path, pdf_info_map)
    base_text = normalize_style(text)
    lines = base_text.splitlines(keepends=True)
    lines = apply_headline_limit(client, lines)
    base_text = "".join(lines)
    if structure_matches_example(client, base_text):
        prefix, sections = split_sections(lines)
        if sections:
            out_lines: List[str] = []
            out_lines.extend(prefix)
            rewritten_any = False
            for key, heading, content_lines in sections:
                if out_lines and out_lines[-1].strip():
                    out_lines.append("\n")
                out_lines.append(heading)
                block_text = "".join(content_lines).strip()
                limit = SECTION_LIMITS.get(key, 0)
                if limit and non_ws_len(block_text) > limit:
                    sys_prompt = SECTION_PROMPTS.get(key, "")
                    if sys_prompt:
                        block_text = rewrite_block(client, block_text, sys_prompt, limit_chars=limit)
                        rewritten_any = True
                if block_text:
                    if not block_text.endswith("\n"):
                        block_text += "\n"
                    out_lines.append(block_text)
            out_text = ensure_section_spacing("".join(out_lines))
            status = "rewritten" if rewritten_any else "copied"
    else:
        out_text = restructure_to_example(client, base_text)
        out_text = ensure_section_spacing(normalize_style(out_text))
        status = "rewritten"

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(out_text, encoding="utf-8")
    return md_path, status


def run() -> None:
    ap = argparse.ArgumentParser("summary_limit")
    ap.add_argument("--input-dir", default=str(Path(DATA_ROOT) / "paper_summary" / "single"))
    ap.add_argument("--out-root", default=str(Path(DATA_ROOT) / "summary_limit"))
    ap.add_argument("--date", default="")
    ap.add_argument("--concurrency", type=int, default=summary_limit_concurrency)
    args = ap.parse_args()

    in_root = Path(args.input_dir)
    if not in_root.exists():
        print(f"[SUMMARY_LIMIT] input dir not found: {in_root}, skip summary_limit", flush=True)
        return

    if args.date:
        in_dir = in_root / args.date
        if not in_dir.exists():
            print(f"[SUMMARY_LIMIT] input dir not found: {in_dir}, skip summary_limit", flush=True)
            return
        date_str = args.date
    else:
        today = today_str()
        candidate = in_root / today
        if candidate.is_dir():
            in_dir = candidate
            date_str = today
        else:
            subdirs = []
            for d in in_root.iterdir():
                if d.is_dir():
                    name = d.name
                    if len(name) == 10 and name[4] == "-" and name[7] == "-":
                        subdirs.append(d)
            if subdirs:
                subdirs.sort(key=lambda p: p.name)
                in_dir = subdirs[-1]
                date_str = in_dir.name
            else:
                in_dir = in_root
                date_str = today

    files = list_md_files(in_dir)
    if not files:
        print(f"[SUMMARY_LIMIT] no md files in {in_dir}, skip summary_limit", flush=True)
        return
    print("============å¼€å§‹ç”Ÿæˆ summary_limit ============", flush=True)

    out_root = Path(args.out_root)
    single_dir = out_root / "single" / date_str
    gather_dir = out_root / "gather" / date_str
    single_dir.mkdir(parents=True, exist_ok=True)

    pdf_info_map = load_pdf_info_map(date_str)

    to_run: List[Path] = []
    for p in files:
        out_path = single_dir / f"{p.stem}.md"
        if out_path.exists():
            continue
        to_run.append(p)

    total = len(to_run)
    if total == 0:
        gather_path = write_gather(single_dir, gather_dir, date_str)
        print(f"[SUMMARY_LIMIT] all files already processed, single_dir={single_dir}", flush=True)
        print(f"[SUMMARY_LIMIT] gather_path={gather_path}", flush=True)
        return

    client = make_client()
    workers = max(1, int(args.concurrency or 0))
    print(f"[SUMMARY_LIMIT] input_dir={in_dir} total={total} concurrency={workers}", flush=True)

    start = time.monotonic()
    done = 0
    empty = 0
    copied = 0
    rewritten = 0

    with ThreadPoolExecutor(max_workers=workers) as ex:
        future_map = {
            ex.submit(process_one, client, p, single_dir / f"{p.stem}.md", pdf_info_map): p
            for p in to_run
        }
        for fut in as_completed(future_map):
            src = future_map[fut]
            try:
                _, status = fut.result()
                if not status:
                    empty += 1
                elif status == "copied":
                    copied += 1
                elif status == "rewritten":
                    rewritten += 1
            except Exception as e:
                print(f"\r[SUMMARY_LIMIT] error on {src.name}: {e!r}", end="", flush=True)
            done += 1
            elapsed = time.monotonic() - start
            rate = done / elapsed if elapsed > 0 else 0.0
            print(f"\r[SUMMARY_LIMIT] progress done={done}/{total} empty={empty} rate={rate:.2f}/s", end="", flush=True)

    print()
    gather_path = write_gather(single_dir, gather_dir, date_str)
    print(
        f"[SUMMARY_LIMIT] stats copied<=950={copied} rewritten={rewritten}",
        flush=True,
    )
    print(f"[SUMMARY_LIMIT] single_dir={single_dir}", flush=True)
    print(f"[SUMMARY_LIMIT] gather_path={gather_path}", flush=True)
    print("============ç»“æŸç”Ÿæˆ summary_limit ============", flush=True)


if __name__ == "__main__":
    run()
